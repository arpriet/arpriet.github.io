[{"authors":["admin"],"categories":null,"content":"Cesar Conejo Villalobos is an actuary graduated from the University of Costa Rica with five years of experience in bank and finance industry. Along this path, he has learned and applied different techniques in backend and frontend data science, especially in data storage infrastructure, efficient computing, data analysis, and prediction algorithms focused on fraud detection. Cesar is also a philosophy lover, a passion that he shares with data science through the study of data ethics and data democratization.\nDuring the 2020-2021 academic year, he will be a master student in the program Statistics for Data Science at University Carlos III of Madrid.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cconejov.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Cesar Conejo Villalobos is an actuary graduated from the University of Costa Rica with five years of experience in bank and finance industry. Along this path, he has learned and applied different techniques in backend and frontend data science, especially in data storage infrastructure, efficient computing, data analysis, and prediction algorithms focused on fraud detection.","tags":null,"title":"Cesar Conejo Villalobos","type":"authors"},{"authors":[],"categories":[],"content":"Abstract The branch of statistics that study the expected duration of time for an event to occur is called survival analysis. The number of events can be one or more. This project reviews nonparametric methods like Kaplan-Meier, Nelson-Aalen, and Cox proportional hazards model. These techniques are applied to the Hard Drive data sets of Backblaze. This application of survival analysis is called failure-time analysis. In this way, the goal is to find the survival probabilities of the hard disks using the data collected by Backblaze in 2019. With the raw data, we create new variables for applying survival models. The major package used for this exercise is survival. For the number of files, it also uses data.table package.\nSummary The global number of hard disks observed during 2019 was 131 448 observations and the number of hard disks with failure was 2 211.\nThe observations are left truncated and right censored. The distribution of age and study time by fail is: \r\rWe can perceive how the density of fail increases until reaching the first year of operations. We can also appreciate the few values from the end of the expected life of hard drives.\nWe applied the techniques for measuring the survival probabilities:\n  Kaplan Meier\n  Nelson Aalen\n  Both methods gives similar values, as you can see in the following figure: \r\rAs a result, the survival probabilities are: \r\rTherefore, we can conclude that there is more than a 90% probability that a hard disk will reach its estimated useful life of 1825 days.\nOn the other hand, using the package simPH, we can build a simulation of the relative hazard based on a comparison with a median age of 497 in hard disk using Cox regression.\n\r\rAs a result, there are more probabilities to fail during the first 497 days. In conclusion, the simulated relative hazards for ages below the median are more than one. This means that hard disks are more likely to fail at a given point in time than hard disks that have worked for 497 days.\n","date":1585178224,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585178224,"objectID":"53a5b7d58b965d736581bf4fe70532aa","permalink":"https://cconejov.github.io/project/surv_analysis_hdd/","publishdate":"2020-03-25T17:17:04-06:00","relpermalink":"/project/surv_analysis_hdd/","section":"project","summary":"Nonparametric methods such as Kaplan-Meier and Nelson-Aalen is applied to the Hard Drive Data of Backblaze.","tags":["Survival Analysis","Nonparametric Modeling","Other"],"title":"Survival Analysis: Hard Drive Reliability Sample","type":"project"},{"authors":[],"categories":[],"content":"Currently, a lot of processes cause information that can be gathered as time series. Predict anomalies in the observations also is crucial for determining changes in business patterns and decision making. Identifying those events in time series is usually complicated, so package like anomalize in R brings effective solutions for identifying outliers observations. In this exercise, we use this package for detecting anomalies in the price of Tesla shares from January 2019 to March 2020.\n","date":1584485581,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584485581,"objectID":"8e60a23935c9163c536ca7b573856b25","permalink":"https://cconejov.github.io/project/outlier_detection/","publishdate":"2020-03-17T16:53:01-06:00","relpermalink":"/project/outlier_detection/","section":"project","summary":"Identifying and predicting anomalies in time series is crucial for decision making. So, we are going to use an option in R for doing the work.","tags":["Anomaly Detection","Unsupervised Algorithms","Time Series"],"title":"Anomaly Detection in Time Series using R","type":"project"},{"authors":[],"categories":[],"content":"On the Internet, you can find a lot of definitions of data science. My preferred description of this science I founded it in the Harvard Data Science Review (HDSR). Rafael A. Irizarry defines data science as \u0026ldquo;an umbrella term to describe the entire complex and multistep processes used to extract value from data\u0026rdquo; (Irizarry, 2020). Following this definition, a strong data scientist require to have expertise in the following areas (Diesinger, 2016):\n Technical skills. Analytical skills. Business skills.  Usually, technical skill is related to informatics and coding abilities. Irizarry calls this area Backend data science or data engineering. Currently, the most popular framework for data science is R and Python (or a mix of both); although, Julia can have a big role in the future. For example, in this tweet, Viral B. Shah explains that Julia is better in High-performance computing than Python.\n\r\rViral B. Shah and Elon Musk tweet\r\r\rAdditionally, technical skills include the capacity to manage various computational architectures, such as databases and operating systems but also other skills such as parallel computing and high performance computing.\nIn summary, the abilities of managing, cleansing, consolidating, and modeling data must be a crucial requirement for data scientists. Also, (Diesinger, 2016) consider that some profiles for data scientists are focused only on this aspect.\nOn the other hand, analytical skills are related more to data analysis. Irizarry denominates this branch Frontend data science. Moreover, this specific area can be divided depending on the tasks within data analysts and machine learning engineers. Data analysts are involved in the process of modeling, simulation, and causal inference. Machine learning engineers design and develop prediction algorithms that need a large amount of data. Usually, both areas require high performance in advance statistics, math, experiment design, research expertise, and data visualization.\nFinally, the goal of data science is to provide business problem solving using scientific approaches. The two previous skills focus on the scientific approach, but data scientists require to have a good perspective of business processes with the purpose of given effective solutions. Also, it is very easy to fall in the false authority fallacy when data scientists use technical terms for expressing their solutions. As a result, \u0026ldquo;it is important for data scientists to communicate effectively with business users utilizing business lingua\u0026rdquo; (Diesinger, 2016).\nReferences   Irizarry, R. A. (2020). The Role of Academia in Data Science Education. Harvard Data Science Review. https://hdsr.mitpress.mit.edu/pub/gg6swfqh\n  Diesinger, P. M. (2016). Data Scientist skill Set. https://d4t4science.com/2016/11/06/data-scientist-skill-set/\n  ","date":1584028762,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584028762,"objectID":"9f7e20d2e77b2fd2921f68a479115dd5","permalink":"https://cconejov.github.io/post/ds_skill_set/","publishdate":"2020-03-12T09:59:22-06:00","relpermalink":"/post/ds_skill_set/","section":"post","summary":"On the Internet, you can find a lot of definitions of data science. My preferred description of this science I founded it in the Harvard Data Science Review (HDSR). Rafael A.","tags":[],"title":"Skill Set for data scientists","type":"post"},{"authors":[],"categories":[],"content":"This project provides some examples of unsupervised algorithms in machine learning. In these techniques, we need to infer the properties of the observations without the help of an output variable or supervisor. We review two methods: k-means and hierarchical clustering. Then we use some data from Kaggle for applying these techniques to produce a customer segmentation. The platform that we use is R. Because of the number of observations, we are going to use a parallel process for improving the execution times using the snow package.\n","date":1583725462,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583725462,"objectID":"62d9c91182f69cacf45310931b26c720","permalink":"https://cconejov.github.io/project/unsupervised/","publishdate":"2020-03-08T21:44:22-06:00","relpermalink":"/project/unsupervised/","section":"project","summary":"In this project, we review two methods of unsupervised techniques: k-means and hierarchical clustering.","tags":["Machine Learning","Unsupervised Algorithms","Clustering"],"title":"Unsupervised Algorithms","type":"project"},{"authors":[],"categories":[],"content":"Machine learning: Supervised learning. Currently, lots of people talk about machine learning and its applications. People consider that the success of companies rests in the opportunity of applying ML techniques in your business. In this way, it is important having some knowledge about these techniques and more importantly how to use these techniques.\nThe idea of this project is to show some of the techniques available in machine learning, and we are going to start with one of the big branches: supervised learning methodologies. Supervised learning scenario occurs when \u0026ldquo;a set of variables that might be denoted as inputs have some influence on one or more outputs\u0026rdquo; (Hastie, Tibshirani, and Friedman, 2009). The purpose of supervised learning is to use the inputs to predict the values of the outputs.\nOn the other hand, depending on the characteristic of the output, we have different problems. If the output is a quantitative variable, we have a regression problem. In case that the output is qualitative, we have a classification problem. However, in the case of some qualitative outcomes with two classes, we can use regression techniques into classification problems.\nClassification The focus of the section is to consider the classification problem, then compare some techniques in one simple exercise and finally to use a cross-validation method for estimating the prediction error. All these steps are made in R program using the following libraries:\nlibrary(tidyverse) #ggplot\rlibrary(kknn) #kknn\rlibrary(e1071) #svm\rlibrary(rpart) #tree\rlibrary(randomForest) #RF\rlibrary(ada) #Boost\rlibrary(caret) #CV\r Using the purchasedBikes file, this table contains 1000 observations of 12 variables. We want to use 11 predictor variables for predict the binary class as output PurchasedBike. It indicates if a customer bought or not a bicycle.\npurchased_bike \u0026lt;- read.table(\u0026quot;purchasedBikes.csv\u0026quot;,\rheader = TRUE,\rsep = \u0026quot;;\u0026quot;,\rdec = \u0026quot;,\u0026quot;,\rrow.names = 1)\r Because the purpose of this exercise is to compare the performance of some techniques, we can see that the outcome variable PurchasedBike is well balanced (48% of the observations in the data set corresponding to Yes value).\n\r\rggplot(data = purchased_bike) +\rgeom_bar(aes( x = PurchasedBike, fill = PurchasedBike)) +\rlabs(title = \u0026quot;Distribution Purchased Bike\u0026quot;)  The goal of this exercise is to compare several classification techniques with this example table. Here, the purpose is to predict Yes value in the PurchasedBike using ten cross-validations with 5 groups with the methods:\n Support Vector Machine KNN Bayes Decision tree Random Forest Boosting methods (AdaBoost)  Furthermore, the two metrics that we are going to use for determining how good our model is are:\n How many \u0026ldquo;yes\u0026rdquo; purchase bike variable the model detects. High values are best. Minimum average global error, calculated as 1 - accuracy, where accuracy is calculated as the number of correct predictions divided by the number of total predictions. Low values are best.  For determining the number of \u0026ldquo;yes\u0026rdquo; in the PurchasedBike and the average global error, we do the following code:\n#Number of observation\rn \u0026lt;- dim(purchased_bike)[1]\r#SVM\ryes_detect_svm \u0026lt;- rep(0,10)\rerror_detect_svm \u0026lt;- rep(0,10)\r#KNN\ryes_detect_knn \u0026lt;- rep(0,10)\rerror_detect_knn \u0026lt;- rep(0,10)\r#Bayes\ryes_detect_bayes \u0026lt;- rep(0,10)\rerror_detect_bayes \u0026lt;- rep(0,10)\r#trees\ryes_detect_tree \u0026lt;- rep(0,10)\rerror_detect_tree \u0026lt;- rep(0,10)\r#RFs\ryes_detect_RF \u0026lt;- rep(0,10)\rerror_detect_RF \u0026lt;- rep(0,10)\r#boosting\ryes_detect_boosting \u0026lt;- rep(0,10)\rerror_detect_boosting \u0026lt;- rep(0,10)\r# cross validation 10 times\rfor(i in 1:10) { #10\rgroups \u0026lt;- createFolds(1:n,5) #5 groups\r#SVM\ryes_svm \u0026lt;- 0\rerror_svm \u0026lt;- 0\r#KNN\ryes_knn \u0026lt;- 0\rerror_knn \u0026lt;- 0\r#Bayes\ryes_bayes \u0026lt;- 0\rerror_bayes \u0026lt;- 0\r#trees\ryes_tree \u0026lt;- 0\rerror_tree \u0026lt;- 0\r#RFs\ryes_RF \u0026lt;- 0\rerror_RF \u0026lt;- 0\r#boosting\ryes_boosting \u0026lt;- 0\rerror_boosting \u0026lt;- 0\r# This for does \u0026quot;cross-validation\u0026quot; with 5 groups (Folds)\rfor(k in 1:5) { #5\rtraining \u0026lt;- groups[[k]] #list\rttesting \u0026lt;- purchased_bike[training,]\rtlearning \u0026lt;- purchased_bike[-training,]\r#SVM\rmodel \u0026lt;- svm(PurchasedBike~., data = tlearning, kernel =\u0026quot;radial\u0026quot;)\rprediction \u0026lt;- predict(model, ttesting)\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_svm \u0026lt;- yes_svm + MC[2,2]\r# Error detection\rerror_svm \u0026lt;- error_svm + (1 - (sum(diag(MC)))/sum(MC))*100\r#KNN\rmodel \u0026lt;- train.kknn(PurchasedBike~., data = tlearning, kmax = 7)\rprediction \u0026lt;- predict(model,ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_knn \u0026lt;- yes_knn + MC[2,2]\r# Error detection\rerror_knn \u0026lt;- error_knn + (1 - (sum(diag(MC)))/sum(MC))*100\r#Bayes\rmodel \u0026lt;- naiveBayes(PurchasedBike~., data = tlearning)\rprediction \u0026lt;- predict(model, ttesting[,-12])\rActual \u0026lt;- ttesting[,12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_bayes \u0026lt;- yes_bayes + MC[2,2]\r# Error detection\rerror_bayes \u0026lt;- error_bayes + (1 - (sum(diag(MC)))/sum(MC))*100\r#trees\rmodel \u0026lt;- rpart(PurchasedBike~. ,data = tlearning)\rprediction \u0026lt;- predict(model, ttesting, type='class')\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_tree \u0026lt;- yes_tree + MC[2,2]\r# Error detection\rerror_tree \u0026lt;- error_tree + (1 - (sum(diag(MC)))/sum(MC))*100\r#RFs\rmodel \u0026lt;- randomForest(PurchasedBike~., data = tlearning, importance=TRUE)\rprediction \u0026lt;- predict(model, ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_RF \u0026lt;- yes_RF + MC[2,2]\r# Error detection\rerror_RF \u0026lt;- error_RF + (1 - (sum(diag(MC)))/sum(MC))*100\r#boosting\rmodel \u0026lt;- ada(PurchasedBike~., data = tlearning, iter=20, nu = 1, type = \u0026quot;discrete\u0026quot;)\rprediction \u0026lt;- predict(model, ttesting[, -12])\rActual \u0026lt;- ttesting[, 12]\rMC \u0026lt;- table(Actual, prediction)\r# \u0026quot;Yes\u0026quot; purchase detection\ryes_boosting \u0026lt;- yes_boosting + MC[2,2]\r# Error detection\rerror_boosting \u0026lt;- error_boosting + (1-(sum(diag(MC)))/sum(MC))*100\r}\r#SVM yes_detect_svm[i] \u0026lt;- yes_svm\rerror_detect_svm[i] \u0026lt;- error_svm/5\r#KNN\ryes_detect_knn[i] \u0026lt;- yes_knn\rerror_detect_knn[i] \u0026lt;- error_knn/5\r#Bayes\ryes_detect_bayes[i] \u0026lt;- yes_bayes\rerror_detect_bayes[i] \u0026lt;- error_bayes/5\r#trees\ryes_detect_tree[i] \u0026lt;- yes_tree\rerror_detect_tree[i] \u0026lt;- error_tree/5\r#RF\ryes_detect_RF[i] \u0026lt;- yes_RF\rerror_detect_RF[i] \u0026lt;- error_RF/5\r#boosting\ryes_detect_boosting[i] \u0026lt;- yes_boosting\rerror_detect_boosting[i] \u0026lt;- error_boosting/5\r}\r Finally, we create three auxiliary tables for showing the results:\ncolors \u0026lt;- c(\u0026quot;SVM\u0026quot; = \u0026quot;blue\u0026quot;,\r\u0026quot;KNN\u0026quot; = \u0026quot;red\u0026quot;,\r\u0026quot;Bayes\u0026quot; = \u0026quot;orange\u0026quot;,\r\u0026quot;Tree\u0026quot; = \u0026quot;purple\u0026quot;,\r\u0026quot;RF\u0026quot; = \u0026quot;black\u0026quot;,\r\u0026quot;Boost\u0026quot; = \u0026quot;green\u0026quot;\r)\rdetect \u0026lt;- tibble(Iteration = seq(1,10),\rSVM = yes_detect_svm,\rKNN = yes_detect_knn,\rBayes = yes_detect_bayes,\rTree = yes_detect_tree,\rRF = yes_detect_RF,\rBoost = yes_detect_boosting)\rglobal_error \u0026lt;- tibble(Iteration = seq(1,10),\rSVM = error_detect_svm,\rKNN = error_detect_knn,\rBayes = error_detect_bayes,\rTree = error_detect_tree,\rRF = error_detect_RF,\rBoost = error_detect_boosting)\r The plot with the yes value detection shows us that the method random forest has on average the best capacity for detecting the positive cases of bikes purchased.\nggplot(data = detect, aes(x = Iteration) ) +\rgeom_line(aes( y = SVM, color = \u0026quot;SVM\u0026quot;) ) +\rgeom_line(aes( y = KNN, color = \u0026quot;KNN\u0026quot;)) +\rgeom_line(aes( y = Bayes, color = \u0026quot;Bayes\u0026quot;)) +\rgeom_line(aes( y = Tree, color = \u0026quot;Tree\u0026quot;)) +\rgeom_line(aes( y = RF, color = \u0026quot;RF\u0026quot;), size = 1) +\rgeom_line(aes( y = Boost, color = \u0026quot;Boost\u0026quot;)) +\rscale_x_continuous(breaks=c(1:10), labels=c(1:10),limits=c(1,10)) +\rlabs(x = \u0026quot;Iteration\u0026quot;,\ry = \u0026quot;Yes detection\u0026quot;,\rcolor = \u0026quot;Method\u0026quot;,\rtitle = \u0026quot;Quantity of Purchased bike variable\u0026quot;\r) +\rscale_color_manual(values = colors)\r \r\rBesides, Random forest also has the lest average global error, so this method also classifies very well the negative case of bike did not purchase.\nggplot(data = global_error, aes(x = Iteration) ) +\rgeom_line(aes( y = SVM, color = \u0026quot;SVM\u0026quot;) ) +\rgeom_line(aes( y = KNN, color = \u0026quot;KNN\u0026quot;)) +\rgeom_line(aes( y = Bayes, color = \u0026quot;Bayes\u0026quot;)) +\rgeom_line(aes( y = Tree, color = \u0026quot;Tree\u0026quot;)) +\rgeom_line(aes( y = RF, color = \u0026quot;RF\u0026quot;), size = 1) +\rgeom_line(aes( y = Boost, color = \u0026quot;Boost\u0026quot;)) +\rscale_x_continuous(breaks=c(1:10), labels=c(1:10),limits=c(1,10)) +\rlabs(x = \u0026quot;Iteration\u0026quot;,\ry = \u0026quot;AVG global Error\u0026quot;,\rcolor = \u0026quot;Method\u0026quot;,\rtitle = \u0026quot;Average global error by iteration\u0026quot;\r) +\rscale_color_manual(values = colors)\r \r\rReferences Hastie T., Tibshirani R., Friedman J. 2008. The elements of Statistical Learning. Springer.\n","date":1583539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583539200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"https://cconejov.github.io/project/internal-project/","publishdate":"2020-03-07T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An introduction to supervised machine learning algorithms, especially methods for classification problems.","tags":["Machine Learning","Supervised Algorithms","Classification"],"title":"Supervised Algorithms","type":"project"}]